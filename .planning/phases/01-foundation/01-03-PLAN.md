---
phase: 01-foundation
plan: 03
type: execute
---

<objective>
Implement the LanguageModelChatProvider interface to register NEAR AI models in VSCode's native model picker.

Purpose: Complete Phase 1 goal - NEAR AI models appearing in VSCode's model picker and responding to chat requests.
Output: Working model provider that streams responses from NEAR AI through VSCode's chat interface.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md (will exist)
@.planning/phases/01-foundation/01-02-SUMMARY.md (will exist)

**From research - LanguageModelChatProvider interface:**
Three required methods:
1. provideLanguageModelChatInformation() - Return available models
2. provideLanguageModelChatResponse() - Handle chat requests with streaming
3. provideTokenCount() - Estimate token counts

**Key implementation details:**
- Vendor ID must match package.json: "near-ai"
- Message conversion: VSCode format -> OpenAI format
- Streaming: Use for-await on OpenAI stream, report via progress.report()
- Cancellation: Check token.isCancellationRequested in stream loop

**Pitfalls from research:**
- Must handle cancellation properly
- Report chunks immediately, don't buffer
- Error handling should surface to user
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement LanguageModelChatProvider</name>
  <files>src/provider/nearAiProvider.ts, src/provider/messageConverter.ts</files>
  <action>
1. **Create src/provider/messageConverter.ts** - Message format conversion:
   ```typescript
   import * as vscode from 'vscode';

   export function convertMessages(
     messages: readonly vscode.LanguageModelChatRequestMessage[]
   ): Array<{ role: 'user' | 'assistant' | 'system'; content: string }> {
     return messages.map(msg => ({
       role: mapRole(msg.role),
       content: extractTextContent(msg.content)
     }));
   }

   function mapRole(role: vscode.LanguageModelChatMessageRole): 'user' | 'assistant' | 'system' {
     switch (role) {
       case vscode.LanguageModelChatMessageRole.User:
         return 'user';
       case vscode.LanguageModelChatMessageRole.Assistant:
         return 'assistant';
       default:
         return 'user';
     }
   }

   function extractTextContent(
     content: readonly (vscode.LanguageModelTextPart | vscode.LanguageModelToolResultPart | vscode.LanguageModelToolCallPart)[]
   ): string {
     return content
       .filter((part): part is vscode.LanguageModelTextPart =>
         part instanceof vscode.LanguageModelTextPart)
       .map(part => part.value)
       .join('');
   }
   ```

2. **Create src/provider/nearAiProvider.ts** - Full provider implementation:

```typescript
import * as vscode from 'vscode';
import OpenAI from 'openai';
import { createNearAiClient, AVAILABLE_MODELS } from '../client/nearAiClient';
import { getAuthFromConfigFile, isAuthConfigured } from '../auth/nearAuth';
import { convertMessages } from './messageConverter';

export class NearAiChatModelProvider implements vscode.LanguageModelChatProvider {
  private client: OpenAI | null = null;

  constructor(private context: vscode.ExtensionContext) {}

  private getClient(): OpenAI {
    if (!this.client) {
      const auth = getAuthFromConfigFile();
      if (!auth) {
        throw new Error('NEAR AI not configured. Run "nearai login" in terminal first.');
      }
      this.client = createNearAiClient(auth);
    }
    return this.client;
  }

  async provideLanguageModelChatInformation(
    options: { silent: boolean },
    token: vscode.CancellationToken
  ): Promise<vscode.LanguageModelChatInformation[]> {
    // Return hardcoded models for now
    // Future: fetch from API
    if (!isAuthConfigured() && !options.silent) {
      vscode.window.showWarningMessage(
        'NEAR AI not configured. Run "nearai login" to authenticate.'
      );
    }

    return AVAILABLE_MODELS.map(model => ({
      id: `near-ai.${model.id}`,
      name: model.name,
      family: model.family,
      version: model.version,
      maxInputTokens: model.maxInputTokens,
      maxOutputTokens: model.maxOutputTokens,
      tooltip: model.tooltip,
      capabilities: {
        toolCalling: false,  // Start without tool calling
        imageInput: false
      }
    }));
  }

  async provideLanguageModelChatResponse(
    model: vscode.LanguageModelChatInformation,
    messages: readonly vscode.LanguageModelChatRequestMessage[],
    options: vscode.ProvideLanguageModelChatResponseOptions,
    progress: vscode.Progress<vscode.LanguageModelResponsePart>,
    token: vscode.CancellationToken
  ): Promise<void> {
    const client = this.getClient();
    const convertedMessages = convertMessages(messages);

    // Map VSCode model ID back to NEAR AI format
    const nearAiModelId = this.mapModelId(model.id);

    try {
      const stream = await client.chat.completions.create({
        model: nearAiModelId,
        messages: convertedMessages,
        stream: true
      });

      for await (const chunk of stream) {
        if (token.isCancellationRequested) {
          // Abort the stream
          stream.controller.abort();
          break;
        }

        const content = chunk.choices[0]?.delta?.content;
        if (content) {
          progress.report(new vscode.LanguageModelTextPart(content));
        }
      }
    } catch (error) {
      const message = error instanceof Error ? error.message : 'Unknown error';
      throw new Error(`NEAR AI request failed: ${message}`);
    }
  }

  async provideTokenCount(
    model: vscode.LanguageModelChatInformation,
    text: string | vscode.LanguageModelChatRequestMessage,
    token: vscode.CancellationToken
  ): Promise<number> {
    // Simple heuristic: ~4 chars per token
    const str = typeof text === 'string'
      ? text
      : convertMessages([text]).map(m => m.content).join('');
    return Math.ceil(str.length / 4);
  }

  private mapModelId(vscodeModelId: string): string {
    // near-ai.fireworks-qwen-72b -> fireworks::accounts/fireworks/models/qwen2p5-72b-instruct
    const model = AVAILABLE_MODELS.find(m => `near-ai.${m.id}` === vscodeModelId);
    return model?.nearAiId ?? vscodeModelId;
  }
}
```

3. **Update src/client/nearAiClient.ts** - Add AVAILABLE_MODELS export:
   ```typescript
   export const AVAILABLE_MODELS = [
     {
       id: 'fireworks-qwen-72b',
       nearAiId: 'fireworks::accounts/fireworks/models/qwen2p5-72b-instruct',
       name: 'Qwen 2.5 72B Instruct',
       family: 'qwen',
       version: '2.5',
       maxInputTokens: 32768,
       maxOutputTokens: 8192,
       tooltip: 'High-performance open model via NEAR AI (Fireworks)'
     }
   ];
   ```

**Avoid:**
- Don't buffer streaming responses - report immediately
- Don't ignore cancellation token
- Don't hardcode auth - use auth module
  </action>
  <verify>
ls -la src/provider/
npm run compile
grep -l "LanguageModelChatProvider" src/provider/nearAiProvider.ts
  </verify>
  <done>
- src/provider/nearAiProvider.ts implements all three methods
- src/provider/messageConverter.ts handles format conversion
- Build succeeds
- No TypeScript errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Register provider in extension activation</name>
  <files>src/extension.ts</files>
  <action>
1. **Update src/extension.ts** to register the provider:

```typescript
import * as vscode from 'vscode';
import { NearAiChatModelProvider } from './provider/nearAiProvider';

export function activate(context: vscode.ExtensionContext) {
  console.log('SpecFlow extension activated');

  // Register the NEAR AI language model provider
  const provider = new NearAiChatModelProvider(context);
  const disposable = vscode.lm.registerLanguageModelChatProvider(
    'near-ai',  // Must match vendor in package.json
    provider
  );

  context.subscriptions.push(disposable);

  // Register management command
  const manageCommand = vscode.commands.registerCommand(
    'specflow.manageNearAi',
    async () => {
      const auth = await import('./auth/nearAuth');
      if (auth.isAuthConfigured()) {
        vscode.window.showInformationMessage(
          'NEAR AI is configured. Config file: ' + auth.getConfigFilePath()
        );
      } else {
        const action = await vscode.window.showWarningMessage(
          'NEAR AI not configured. Run "nearai login" in terminal to authenticate.',
          'Open Terminal'
        );
        if (action === 'Open Terminal') {
          const terminal = vscode.window.createTerminal('NEAR AI Setup');
          terminal.show();
          terminal.sendText('nearai login');
        }
      }
    }
  );

  context.subscriptions.push(manageCommand);
}

export function deactivate() {}
```

2. Verify vendor ID "near-ai" matches package.json contribution.

**Avoid:**
- Don't do async work in activate() synchronously - registration is sync
- Don't forget to add to context.subscriptions for cleanup
  </action>
  <verify>
npm run compile
grep -l "registerLanguageModelChatProvider" src/extension.ts
  </verify>
  <done>
- Provider registered in activate()
- Management command registered
- Build succeeds
- Vendor ID matches package.json
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete NEAR AI model provider integrated with VSCode's model picker</what-built>
  <how-to-verify>
1. Press F5 in VSCode to launch Extension Development Host
2. Open a new chat (Ctrl+Shift+I or View > Chat)
3. Click the model picker dropdown in the chat input area
4. Look for "NEAR AI" section with "Qwen 2.5 72B Instruct" model
5. If models don't appear:
   - Check Output > "SpecFlow" for errors
   - Run "SpecFlow: Manage NEAR AI Connection" command
   - Ensure you have a Copilot subscription (Free/Pro/Pro+) - required for custom providers
6. If models appear, select one and send a test message: "Hello, what model are you?"
7. Verify streaming response appears (not all at once)

**Expected outcome:**
- NEAR AI models appear in model picker
- Selecting model and sending message returns streaming response
- OR clear error message if auth not configured
  </how-to-verify>
  <resume-signal>Type "approved" if models work, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run compile` succeeds
- [ ] Provider implements all three LanguageModelChatProvider methods
- [ ] Provider registered with correct vendor ID
- [ ] Management command registered
- [ ] Human verified models appear in picker OR clear error for missing auth
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- NEAR AI models appear in VSCode model picker (or clear auth error)
- Streaming responses work
- Phase 1 complete - ready for Phase 2 (Chat Participant)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md` with:
- What was built
- Whether models appeared in picker
- Any issues encountered
- Phase 1 completion status
</output>
